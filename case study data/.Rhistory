library(kohonen) # SOM
library(aweSOM) # quality metrics: quantization error, topo error
library(lhs) # latin hypercube design of SOM parameters
library(clusterSim)
rm(list=ls())
source("G:/My Drive/CU Boulder/CRB publications/Uncertainty characterization and RDM sensitivity paper/R/Library.R")
# Import DV
setwd('G:/My Drive/CU Boulder/CRB publications/Uncertainty characterization and RDM sensitivity paper/R/data')
#setwd('G:/My Drive/CU Boulder/CRB publications/Uncertainty characterization and RDM sensitivity paper/R/data')
Archive='Archive_463_Condensed.txt'
DV=read.table(Archive, header = T, sep = "")
#### k means clustering to get a sense for how many nodes we might want
# SUBSET of 90 policies
KI=which(optimization$Mead.1000 <= 10 & optimization$Powell.3490 <= 5 & optimization$Lee.Ferry.Deficit <= 1)
filter.optimization=scale(optimization[KI, -1])
DBindex=2:45
set.seed(7)
for(k in 2:45){
temp=kmeans(x = filter.optimization, centers = k, nstart = 25)
cluster=temp$cluster
temp=index.DB(x = filter.optimization,cl = cluster)
DBindex[k-1]=temp$DB
}
DB.df=data.frame(k=2:45, DBindex)
ggplot(data = DB.df, aes(x=k, y=DBindex))+
geom_line()+
ggtitle("Davies-Bouldin index for optimization objectives")+
ylab("index")+
xlab("clusters")+
scale_x_continuous(breaks=seq(0,100, by=4))+
geom_point(x=5, y=DB.df$DBindex[4], size=3, color="blue")+
geom_text(x=5, y=DB.df$DBindex[4]-.01, label="k=5")
# conclusion: Davies Bouldin trends downward with no clear optimum. However, for k < 10, k = 5 is the clear winner.
######################################################################
########################## lhs #######################################
n_samples=1000
# testing radius, grid size, distance function, neighborhood function, toroidal or planar
set.seed(7)
#unit_cube=geneticLHS(n=n_samples, k=4, criterium = 'Maximin') # optimizes Maximin optimality criterion.s
unit_cube=improvedLHS(n=n_samples, k=5) # optimizes for euclidean distance between points
unit_cube=data.frame(unit_cube)
colnames(unit_cube)=c("radius", "n_nodes", "distance_fnc", "neighbor_fnc", "toroidal")
# convert to uniform sample across given ranges
radius=c(0,1) # as a fraction of max unit-to-unit distances
n_nodes=c(4, 25)
distance_fnc=c("sumofsquares", "euclidean", "manhattan")
neighbor_fnc=c("bubble", "gaussian")
toroidal=c(T, F)
cube=unit_cube
cube$radius=runif(unit_cube$radius, min=radius[1], max=radius[2])
cube$n_nodes=round(runif(unit_cube$n_nodes, min=n_nodes[1], max=n_nodes[2]))
### transform distance functions to discrete
probs=seq(0,1, length.out = (length(distance_fnc)+1)) # sets equal probability ranges for finding which 0 to 1 values correspond to given distance function
for (i in 1:(length(probs)-1)){
cube$distance_fnc[which(unit_cube$distance_fnc > probs[i] & unit_cube$distance_fnc<= probs[i+1])]=distance_fnc[i]
}
### transform neighbor functions and toroidal TF to discrete
probs=seq(0,1, length.out = (length(neighbor_fnc)+1)) # sets equal probability ranges for finding which 0 to 1 values correspond to given neighbor function
for (i in 1:(length(probs)-1)){
cube$neighbor_fnc[which(unit_cube$neighbor_fnc > probs[i] & unit_cube$neighbor_fnc<= probs[i+1])]=neighbor_fnc[i]
cube$toroidal[which(unit_cube$toroidal > probs[i] & unit_cube$toroidal<= probs[i+1])]=toroidal[i]
}
## test that each value is equally represented in the hypercube
fraction=1:length(distance_fnc) # preallocate
c=1
for (i in distance_fnc){
fraction[c]=sum(cube$distance_fnc==i)/nrow(cube)
c=c+1
}
fraction # each element in fraction should be equal. It is the fraction of your samples that correspond to each demand value
fraction=1:length(neighbor_fnc) # preallocate
c=1
for (i in neighbor_fnc){
fraction[c]=sum(cube$neighbor_fnc==i)/nrow(cube)
c=c+1
}
fraction
fraction=1:length(toroidal) # preallocate
c=1
for (i in toroidal){
fraction[c]=sum(cube$toroidal==i)/nrow(cube)
c=c+1
}
fraction
##################################################################################################################
############################## calculate SOM map and report quality metrics for each #############################
scaled_data= filter.optimization
# Kohonen 2013 - The Essentials of the SOm - section 3.5.
# "It is advisable to select the lengths of the horizontal and vertical dimensions of the array
# to correspond to the lengths of the two largest principal components (ie those with the highest
# eigenvalues of the input correlation matrix)". So, I calculate the covariance matrix, which is square,
# then compute the eigenvalues, then take ratio of top two
temp.cov=cov(scaled_data)
temp.eigenValues=eigen(temp.cov)$values
topEV=sort(temp.eigenValues,decreasing = T)[1:2]
ratio=topEV[1]/topEV[2]
# create initialization matrix as uniformly sampled plane along first two PCs. See Kohonen 2013 and Clark et al 2013
X=scaled_data
temp.eigenVectors=eigen(cov(X))
RM=temp.eigenVectors$vectors[,1:2] # rotation matrix, which is the eigenvectors corresponding to first and second largest eigenvalues
PCs=X %*% RM
PC1range=range(PCs[,1]) # will use these to sample uniformly along PC1 and PC2 within the for loop
PC2range=range(PCs[,2])
metric.df=matrix(nrow=n_samples, ncol= 4) # preallocate matrix to store quality of fit metrics
metric.df=data.frame(metric.df)
params.df=matrix(nrow=n_samples, ncol=6)
params.df=data.frame(params.df)
colnames(params.df)=c("x_dim", "y_dim", "radius", "dist_fnc", "neighbor_fnc", "toroidal")
View(scaled_data)
